services:
  nginx:
    image: nginx:latest
    container_name: nginx-ingress
    ports:
      - "${NGINX_HTTP_PORT:-80}:80"
      - "${NGINX_HTTPS_PORT:-443}:443"
      - "${NGINX_PORT:-8888}:8888"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./config/ssl/certs:/etc/nginx/ssl:ro
      - certbot_webroot:/var/www/certbot:ro
      - certbot_certs:/etc/letsencrypt:ro
    depends_on:
      open-webui:
        condition: service_healthy
      langflow:
        condition: service_healthy
      code-server:
        condition: service_started
      app:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/ || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # Certbot for Let's Encrypt certificates (optional - run manually when needed)
  # Usage: docker compose run --rm certbot certonly --webroot -w /var/www/certbot -d yourdomain.com
  certbot:
    image: certbot/certbot:latest
    container_name: certbot
    volumes:
      - certbot_webroot:/var/www/certbot
      - certbot_certs:/etc/letsencrypt
      - ./config/ssl/certs:/output
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done'"
    profiles:
      - ssl

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./config/ollama:/config/ollama:ro
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_NUM_THREAD=8
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MODELS_CONFIG=/config/ollama/models.json
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"  # Expose for direct access and health checks
    volumes:
      - open-webui_data:/app/backend/data
      - ./config/open-webui/presets.json:/app/backend/data/presets.json:ro
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-supersecret}
      - RAG_EMBEDDING_MODEL=nomic-embed-text
      - RAG_TOP_K=5
      - DEFAULT_MODELS=qwen2.5-coder:7b-q4_0
      - ENABLE_RAG_WEB_SEARCH=true
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # Stable Diffusion WebUI - Optional, uncomment to enable
  # Uses AUTOMATIC1111's WebUI for image generation
  # stable-diffusion:
  #   image: ghcr.io/cmdr2/stable-diffusion-ui:2.6.2
  #   container_name: stable-diffusion
  #   runtime: nvidia
  #   ports:
  #     - "7861:9000"
  #   volumes:
  #     - sd_data:/app/data
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   environment:
  #     - WEBUI_ARGS=--listen --api
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -sf http://localhost:9000/ || exit 1"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 120s
  #   restart: unless-stopped

  langflow:
    image: langflowai/langflow:latest
    container_name: langflow
    ports:
      - "7860:7860"
    volumes:
      - langflow_data:/app/.langflow
      - ./src:/app/src
      - ./config/langflow/flows:/app/flows:ro
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - LANGFLOW_LOAD_FLOWS_PATH=/app/flows
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:7860/ || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 10
      start_period: 120s
    restart: unless-stopped

  code-server:
    image: linuxserver/code-server:latest
    container_name: code-server
    ports:
      - "${CODE_SERVER_PORT:-8443}:8443"
    volumes:
      - ./src:/config/workspace
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - PUID=1000
      - PGID=1000
      - PASSWORD=${PASSWORD}
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8443/ || exit 1"]
      interval: 20s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  app:
    build: .
    container_name: coding-assistant
    ports:
      - "8000:8000"
      - "8001:8001"  # Metrics endpoint
    volumes:
      - ./src:/app/src
      - chroma_db:/app/chroma_db
      - ./zephyr:/app/zephyr
      - ./insights:/app/insights
      - ./personas.yaml:/app/personas.yaml:ro
      - ./config:/app/config:ro
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - CHROMA_DB_DIR=/app/chroma_db
      - MCP_CONFIG_PATH=/app/config/mcp/servers.json
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - METRICS_PORT=8001
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import sys; sys.exit(0)'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    # Keep container alive for interactive use; override cmd in production
    stdin_open: true
    tty: true
    command: /bin/bash
    restart: unless-stopped
    networks:
      - default
      - ai-monitoring

  docs:
    # Documentation: Self-hosted MkDocs site with Material theme
    # Provides comprehensive documentation and API reference
    # Auto-reloads on documentation source changes
    build:
      context: .
      dockerfile: Dockerfile.docs
    container_name: docs-site
    ports:
      - "8001:8001"
    volumes:
      # Mount documentation sources for live reload
      - ./docs:/docs/docs
      - ./mkdocs.yml:/docs/mkdocs.yml
      # Mount Python source for API documentation extraction
      - ./src:/docs/src:ro
    # Healthcheck ensures documentation server is responsive
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8001 2>/dev/null | grep -q 'MkDocs' || wget -qO- http://localhost:8001 2>/dev/null | grep -q 'Codemate'"]
      interval: 30s
      timeout: 10s
      retries: 3
    # Minimal resource requirements for static documentation
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

volumes:
  ollama_data:
  chroma_db:
  open-webui_data:
  sd_data:
  langflow_data:
  certbot_webroot:
  certbot_certs:

networks:
  default:
    name: ai-research-default
  ai-monitoring:
    external: true
