services:
  ollama:
    # Ollama: Local LLM inference engine
    # Provides model inference API used by app and Langflow
    # Image: Latest stable Ollama (pinning strategy in README)
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      # Persist downloaded models across container restarts
      - ollama_data:/root/.ollama
    # Optional: uncomment next line if NVIDIA GPU available (requires nvidia-docker)
    # runtime: nvidia
    environment:
      # Uncomment if using GPU: - NVIDIA_VISIBLE_DEVICES=all
      # Model inference parallelism (CPU tuning for quad-core)
      - OLLAMA_NUM_PARALLEL=4
      # Thread count per model (CPU tuning for quad-core)
      - OLLAMA_NUM_THREAD=4
      # Explicit binding address (localhost only, accessed via container network)
      - OLLAMA_HOST=0.0.0.0:11434
    # Healthcheck ensures Ollama is responsive before dependent services start
    # Verifies /api/tags endpoint (lists available models)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    # Resource limits prevent container from consuming all host resources
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

  langflow:
    image: langflowai/langflow:latest
    container_name: langflow
    ports:
      - "7860:7860"
    volumes:
      - ./langflow_data:/app/langflow/.langflow
      - ./src:/app/src
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - LANGFLOW_DATABASE_URL=sqlite:///./langflow_data/langflow.db
      - OLLAMA_BASE_URL=http://ollama:11434

  code-server:
    image: linuxserver/code-server:latest
    container_name: code-server
    ports:
      - "8080:8443"
    volumes:
      - ./src:/config/workspace
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - PUID=1000
      - PGID=1000
      - PASSWORD=${PASSWORD}
    depends_on:
      - app

  app:
    build: .
    container_name: coding-assistant
    ports:
      - "8000:8000"
    volumes:
      - ./src:/app/src
      - chroma_db:/app/chroma_db
      - ./zephyr:/app/zephyr
      - ./insights:/app/insights
    depends_on:
      ollama:
        condition: service_healthy
      langflow:
        condition: service_started
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - CHROMA_DB_DIR=/app/chroma_db
    # Keep container alive for interactive use; override cmd in production
    stdin_open: true
    tty: true
    command: /bin/bash

  docs:
    # Documentation: Self-hosted MkDocs site with Material theme
    # Provides comprehensive documentation and API reference
    # Auto-reloads on documentation source changes
    build:
      context: .
      dockerfile: Dockerfile.docs
    container_name: docs-site
    ports:
      - "8001:8001"
    volumes:
      # Mount documentation sources for live reload
      - ./docs:/docs/docs
      - ./mkdocs.yml:/docs/mkdocs.yml
      # Mount Python source for API documentation extraction
      - ./src:/docs/src:ro
    # Healthcheck ensures documentation server is responsive
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8001 2>/dev/null | grep -q 'MkDocs' || wget -qO- http://localhost:8001 2>/dev/null | grep -q 'Codemate'"]
      interval: 30s
      timeout: 10s
      retries: 3
    # Minimal resource requirements for static documentation
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

volumes:
  ollama_data:
    # Persistent storage for Ollama model artifacts
    # Retention: Protected models are kept indefinitely
    # Cleanup: Use scripts/model-prune.sh to remove unused models
    # Size management: Monitor with 'docker system df'
  chroma_db:
    # Persistent storage for Chroma vector database
    # Contains embedding vectors for code context and memory
    # Retention: Preserved across container restarts
    # Cleanup: Remove entire directory to reset embeddings

